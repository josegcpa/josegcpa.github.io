<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://josegcpa.net/feed.xml" rel="self" type="application/atom+xml"/><link href="https://josegcpa.net/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-09T10:29:25+00:00</updated><id>https://josegcpa.net/feed.xml</id><title type="html">blank</title><subtitle>Jos√© is a clinical data scientist and ML engineer. Their hobbies include reading, web dev and wondering what to have for dinner. </subtitle><entry><title type="html">Thinking critically about teaching critical thinking</title><link href="https://josegcpa.net/blog/2026/critical-thinking/" rel="alternate" type="text/html" title="Thinking critically about teaching critical thinking"/><published>2026-01-09T00:00:00+00:00</published><updated>2026-01-09T00:00:00+00:00</updated><id>https://josegcpa.net/blog/2026/critical-thinking</id><content type="html" xml:base="https://josegcpa.net/blog/2026/critical-thinking/"><![CDATA[<p>With an encroaching world of fake news and disinformation, critical thinking has taken the center stage. We hear about how it is oftentimes absent across the education spectrum, and come to terms with how sometimes we can fall prey to rushed and ill-formed conclusions.</p> <p>With that in mind, it is no wonder that critical thinking became a sought after skill for intelectual and creative labour.</p> <p>It is also no wonder that a never-ending flurry of critical thinking courses appeared, ready to teach a new generation of workers and executives on the intricacies of well-structured and inquisitive thinking.</p> <p>It is also no wonder that you might be wondering whether critical thinking can be taught. I certainly was.</p> <p>So I started reading on the topic. Researching. Matching claims to sources. Checking whether what some people say is confirmed by what we know.</p> <p>In other words, thinking critically about the topic.</p> <p>But what I found is that this way of critical thinking is, in essence, scientific thinking. In fact, critical thinking, as a general skill, is not really something you can teach - critical thinking skills learned on a subject are remarkably hard to transfer to other fields.</p> <p>Why? Thinking critically about something requires knowledge on the topic - it requires you to be able to tell what is and isn‚Äôt likely, what are the reasonable sources, how research is performed in this field. There are some standard ‚Äúmeta-strategies‚Äù: check if the sources are reliable, whether your conclusions are rushed or overconfident, whether you might be ignoring evidence to suit your conclusions, etc. They all require knowledge on the topic.</p> <p>Daniel Willingham is an oftentimes cited expert on the topic. His belief is that critical thinking <em>can</em> be taught. It can be taught within subjects, along the years. He defends a pedagogy of critical thinking that spans courses and ages. This comprehensive way of teaching critical thinking stems from a crucial observation: critical thinking can be taught, but not as a skill. In other words: there is critical thinking within subject matters, but there is not a general critical thinking skill which can be applied transversally.</p> <p>The literature closely aligns with this - while there are tools which can be broadly applied, people have a hard time transferring them to new fields and topics. Two situations in particular can lead to a more systematic skill transfer: i) when people become very familiar with <em>the structure of specific problems</em> they have an easier time transferring that structure to other problems and ii) when people are <em>told</em> that a new problem is analogous to an older problem they had to solve. And even when you understand you should transfer skills from one problem to another, you still require the knowledge allowing you to do so.</p> <p>Having said this, I am not surprised people assume critical thinking can be taught as a general skill. The popular tropes we have for engineers and tech founders thinking they can ‚Äúsolve‚Äù everything through abstraction (the engineer syndrome), and for economists trying to model everything by getting rid of anything that‚Äôs hard to quantify (the ‚ÄúRicardian vice‚Äù as Schumpeter puts it) allude to this. People are enamored with the concept of multidisciplinary approaches, but they don‚Äôt want to pay for the people implementing them. Anything promising a broadly applicable and valuable solution will raise a lot of support. Maybe it should raise eyebrows instead.</p> <p>In short:</p> <ol> <li>Critical thinking is remarkably <strong>complicated to teach as a general skill</strong>. Doing so apparently requires long-term solutions and a lot of practice</li> <li>The <strong>idea of the general critical thinker is remarkably appealing</strong>: small teams with the capacity to solve multiple problems are cheaper than larger teams with the same problem solving capabilities. However, reality does not bend to appeal or earnings calls</li> <li>There is a certain <strong>familiarity with problem structures that can help in transferring methods across fields</strong>. Fans of P√≥lya‚Äôs <em>How to solve it</em> (a really good book) will certainly smile and say <em>‚Äúwell, duh‚Äù</em> after reading this. In fact, P√≥lya, as an educator, believed that it was practice and abstraction which could help students - and future mathematicians - solve increasingly complex problems. This allows people to learn or construct new heuristics for problem solving and understand when/where they can be applied. Of course that his method focuses on mathematics, so there is no guarantee that he would vouch for a general critical thinking skill</li> <li><strong>We should be very wary of blindly transferring solutions from field A to field B</strong> without checking in with field B experts about the reasonability and relevance of that solution. A solution <em>fixing</em> a problem is one thing, a solution <em>looking</em> for a problem is entirely different. Tukey put it best: an approximate answer to the right problem is worth more than an exact answer to an approximate problem. Misinterpreting a field oftentimes leads to approximate problems whose solution will have no impact on the field.</li> </ol> <p>Some final remarks: I don‚Äôt have anything against the project of teaching critical thinking: I think it‚Äôs a tremendously valuable skill and that it should be taught more consistently during compulsory education and across different subjects. However, I don‚Äôt see short-term courses on general critical thinking as something particularly valuable. The best case scenario: you get some neat strategies which will work in specific scenarios. And don‚Äôt get me wrong, that can be really helpful! But that‚Äôs not the same as developing <em>actual</em> critical thinking: that is cultivated over time through practice, reflection and deep knowledge on the fields where you want your critical thinking abilities to shine.</p> <p>Sources:</p> <ul> <li>From Mauricio Shiroma: <a href="https://www.cambridge.org/elt/blog/2022/09/07/what-critical-thinking-is/">A deep dive into critical thinking (part 1) ‚Äì what is it and how is it taught?</a> (2022)</li> <li>From Daniel Willingham: <a href="https://education.nsw.gov.au/teaching-and-learning/education-for-a-changing-world/thinking-skills/how-to-teach-critical-thinking">How to teach critical thinking</a> (2019), <a href="https://www.readingrockets.org/topics/comprehension/articles/critical-thinking-why-it-so-hard-teach">Critical Thinking: why is it so hard to teach?</a> (2007), <a href="https://www.aft.org/ae/fall2020/willingham">Ask the Cognitive Scientist: How Can Educators Teach Critical Thinking?</a> (2020)</li> <li>From Althea Need Kaminske: <a href="https://www.learningscientists.org/blog/2019/2/28/can-we-teach-critical-thinking">Can We Teach Critical Thinking?</a> (2019)</li> </ul>]]></content><author><name></name></author><category term="science"/><summary type="html"><![CDATA[Is a pedagogy of critical thinking possible? I tried to find out]]></summary></entry><entry><title type="html">Workshop on medical image segmentation</title><link href="https://josegcpa.net/blog/2025/scope-coimbra/" rel="alternate" type="text/html" title="Workshop on medical image segmentation"/><published>2025-11-27T00:00:00+00:00</published><updated>2025-11-27T00:00:00+00:00</updated><id>https://josegcpa.net/blog/2025/scope-coimbra</id><content type="html" xml:base="https://josegcpa.net/blog/2025/scope-coimbra/"><![CDATA[<p>Me and Alexandre Calado had the opportunity to teach a small course on medical image segmentation at a small student conference organised by the N√∫cleo de Estudantes do Departamento de F√≠sica at Universidade de Coimbra.</p> <p>It was a great opportunity to brush up on what we know and how we can best communicate it. Given that it was supposed to be a hands-on workshop, I thought that a small Kaggle notebook could be a great opportunity of getting everyone‚Äôs hands dirty with a club classic - the Medical Segmentation Decathlon. So I coded a small tutorial on <a href="https://www.kaggle.com/code/josegcpa/medical-decathlon-starter">medical image segmentation</a>. It‚Äôs free to use with an Apache License 2.0, so feel free to use it with the appropriate attribution ü§ó</p>]]></content><author><name></name></author><category term="teaching"/><summary type="html"><![CDATA[teaching university students about medical image segmentation in Python]]></summary></entry><entry><title type="html">Implementing academ.ai, a local retrieval system for academic papers</title><link href="https://josegcpa.net/blog/2025/academai/" rel="alternate" type="text/html" title="Implementing academ.ai, a local retrieval system for academic papers"/><published>2025-09-16T00:00:00+00:00</published><updated>2025-09-16T00:00:00+00:00</updated><id>https://josegcpa.net/blog/2025/academai</id><content type="html" xml:base="https://josegcpa.net/blog/2025/academai/"><![CDATA[<p><strong>tl;dr:</strong> I developed a semantic search engine that runs locally and comes packaged with a nice and simple interface. I used it to look for papers across bioRxiv and medRxiv. The code is open source and is available <a href="https://github.com/josegcpa/academic-search">here</a>.</p> <h1 id="context">Context</h1> <p>An issue which a lot of researchers run into are the relatively outdated search engines powering academic manuscript databases. I felt a bit frustrated with how the relatively poor support for semantic searchers: oftentimes, I got stuck having to think of the ideal keywords to retrieve the papers I was looking for. On the other hand, popular search engines have widely adopted everything related to semantic search (the comically subpar ‚ÄúAI summaries‚Äù will remain undiscussed here).</p> <p>While this is perhaps not something that a lot of people contend with, the usual academic manuscript search rests on a core principle: exactness. If people conduct a proper meta-analysis or meta-review, they will oftentimes post long, intractable queries which are understood by search engines but which can be relatively hard to comprehend. Words can be, for better or worse, both polysemous - where the same word refers to multiple things - and synonymous - where the same thing can be referred to by multiple words. This means that queries have to be highly sensitive (they have to retrieve all papers related to a given field) but also specific (they have to filter out as many unrelated papers as possible).</p> <h2 id="thinking-about-retrieval">Thinking about retrieval</h2> <p>With this in mind I started thinking - can this be improved?</p> <p><strong>Manual query construction</strong> can be quite time-consuming and error-prone. I remember reviewing some meta-analysis papers which missed key papers because they did not think about specific keywords. Some improvements can be considered: preprocessing words and reducing them to their stems (think of the word ‚Äúrunning‚Äù and ‚Äúruns‚Äù - they are both reduced to ‚Äúrun‚Äù) or enriching the query with synonyms. However, we once again run into the issue of exactness as this changes the sensitivity and specificity of our run.</p> <p>I have toyed with <strong>automatic query generation</strong> using LLMs and this works to a good extent. In other words, you can use an LLM to construt queries which can be used by your favourite search engine using a mixture of clever prompting and structured outputs for parsing at a later stage. However, this still relies on some form of exactness which is hard to verify. Different and shifting word meanings can have an impact on how capable LLMs are of building these sorts of queries.</p> <p>We can also consider approaches which first retrieve a very high number of results (high sensitivity) and perform some metadata-based filtering to increase the specificity. However, this is just an iteration of query generation or construction, with or without filters.</p> <p>One of the features I enjoy the most about <strong>modern chatbots</strong> is their ability to search the web. What they do is fairly simple and is, essentially, an iteration of tool use with <strong>automatic query generation</strong>. A search is triggered because the chatbot predicts that its knowledge base is insufficient to provide an accurate answer to the user‚Äôs message, which is converted into a search query. The digestion that happens afterwards, where the relevant information is extracted, is really the best part as it provides some structure to the chatbot‚Äôs output. Now, don‚Äôt get me wrong - this summarization feature is not the most accurate, but it helps me filter through the clutter of web search results.</p> <p>Other recent methods also include the <strong>automatic extraction of metadata</strong> from documents or webpages to facilitate search - however, this is also hindered by having to specify the metadata fields to extract, which can change with time.</p> <p>The ideal would be a method which can take characterize the <em>meaning</em> of a query and retrieve relevant documents to it. This is what <strong>semantic search</strong> is all about.</p> <h2 id="its-all-semantics-non-derogatory">It‚Äôs all semantics (non-derogatory)</h2> <p>Generally, the whole point of <strong>retrieval</strong> is to produce a set of results which are related to a given query. In other words, a good set of results is characterized by proximity to the query in terms of meaning, not exactness. This is where <strong>semantic search</strong> comes in.</p> <p>Semantic search - as it is performed currently - usually uses a natural language processing (NLP) model to quantify a sentence or document as a vector (which can also be referred to as an embedding). This vector should represent (using typically a large number of dimensions) the meaning of that sentence or document. This means that two semantically similar but distinct phrases - ‚Äúthe queen of England‚Äù and ‚Äúthe monarch of the United Kingdom‚Äù - should yield similar vectors. <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p> <p>The options to compute embeddings are manifold and generally involve generating one embedding for each token (which typically corresponds to a word or to a part of a word). The simpler options are models similar to <a href="https://research.google/blog/open-sourcing-bert-state-of-the-art-pre-training-for-natural-language-processing/">BERT</a>, which use a model training paradigm combining masked word prediction (a word at random is masked and predicted from the rest of the sentence) with next sentence prediction (the model predicts whether a given sentence is the next sentence in a document) to guarantee both grammatical and semantic coherence. Other popular methods include <a href="https://arxiv.org/abs/2004.12832">ColBERT</a> - which is similar to BERT but uses a different training paradigm where the model is trained to retrieve the best matching tokens or words from query and document embeddings - and any old LLM which can be used to generate embeddings for a set of tokens. These models yield one embedding for each token. These are aggregated in some way (average or maximum pooling, classification tokens) to yield a single query/sentence/document embedding.</p> <p>On top of this, exact matching (or a <a href="https://www.geeksforgeeks.org/nlp/what-is-bm25-best-matching-25-algorithm/">variation</a>) can still be used - the combination of multiple retrieval methods is typically referred to as ‚Äúhybrid‚Äù search, and most search engines tend towards hybrid searches. While somewhat odd, it starts to make sense once you realise that a lot of people (myself included) use search terms in a relatively telegraphic manner: if I want to search for papers on caffeine‚Äôs impact on the onset of Alzheimer‚Äôs disease, I will simply type ‚Äúcaffeine alzheimer‚Äôs onset‚Äù. Combining both exact matching and semantic search can yield a search that feels more natural and that better works for everyone.</p> <h1 id="implementing-a-local-retrieval-system">Implementing a local retrieval system</h1> <p><strong>Note:</strong> all of this is somewhat similar to what Semantic Scholar does through their API services - I am not sure how their search functionalities work, but I would assume they rely on keyword matching with a minimal amount of semantic search. What I present below is, in essence, the hacker‚Äôs version of that with fewer resources/papers. In any case this is scalable using the <a href="https://www.semanticscholar.org/product/api/tutorial">Semantic Scholar data API</a>.</p> <h2 id="1-getting-the-data">1. Getting the data</h2> <p>To implement a local retrieval system for academic literature search, we first need to get a hold of a dataset of academic abstracts. Luckily, both <a href="https://www.biorxiv.org/">bioRxiv</a> and <a href="https://www.medrxiv.org/">medRxiv</a> have very handy APIs for this. Using programmatic access and some minimal parallelization, we can retrieve a large number of abstracts in a matter of minutes. I stored these in a <a href="https://sqlite.org/index.html">SQLite</a> database so everything is standardized.</p> <h2 id="2-embedding-the-data">2. Embedding the data</h2> <p>This is the time consuming bit. I quite like the <a href="https://huggingface.co/">HuggingFace</a> stack with <a href="https://www.sbert.net/">SentenceTransformers</a>, so I used that for embedding my abstract chunks (sentences or pieces of the abstracts). Given that I am running all of this locally and that I don‚Äôt have a GPU, I used a<a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">really small model</a> for this. For the embedding database/retrieval workhorse I made use of <a href="https://weaviate.io/">Weaviate</a>, a pretty powerful solution that is open source and guarantees fast retrieval while also supporting multiple types of search and metadata filtering.</p> <p>I defined two separate databases (or collections): one for abstracts and the other for abstract chunks. This allowed me to have databases which are less disk space-intensive while guaranteeing that I can retrieve the chunks I need. These chunks are cross-referenced to their abstracts, so it is really easy to retrieve the abstracts corresponding to the chunks.</p> <h2 id="3-defining-an-api">3. Defining an API</h2> <p>The simplest API I could define was one with three endpoints:</p> <ol> <li><code class="language-plaintext highlighter-rouge">/health</code> - a simple health check endpoint</li> <li><code class="language-plaintext highlighter-rouge">/query</code> - a search endpoint which takes a query and returns a structured list of relevant abstract chunks</li> <li><code class="language-plaintext highlighter-rouge">/count</code> - a count endpoint which takes a query and returns the number of relevant abstract chunks by source (i.e. <code class="language-plaintext highlighter-rouge">medrxiv</code> or <code class="language-plaintext highlighter-rouge">biorxiv</code>)</li> </ol> <p>The technical stack here is very classic if you are used to Python - <a href="https://fastapi.tiangolo.com/">FastAPI</a> and <a href="https://pydantic-docs.helpmanual.io/">Pydantic</a> for data models.</p> <h2 id="4-re-ranking-results">4. Re-ranking results</h2> <p>Before we can display the results to the user, we need to re-rank them. While semantic search tends to work well for the general task of retrieving similar results, distances are not always a good proxy for relevance. This is where result re-ranking comes into play: using a set of models known as cross-encoders, which produce a similarity score for two queries (in this case, the abstract chunk and the user‚Äôs query), we can re-rank the results based on actual similarity to the query.</p> <p>Once again, SentenceTransformers came to the rescue - this time with <a href="https://huggingface.co/cross-encoder/ms-marco-TinyBERT-L2-v2">a really small cross-encoder model</a>.</p> <h2 id="5-making-a-pretty-thing">5. Making a pretty thing</h2> <p>I am a fan of minimal web development, so I coded a small frontend using the classic HTML/CSS/JS stack. I know the cool kids are all using React and Tailwind, but it would be an overkill for such a small project. Plus, if it becomes a necessity I can always switch to a more modern stack.</p> <h1 id="using-a-local-retrieval-system">Using a local retrieval system</h1> <p>Given that these are, in essence, a small set of micro-services, there are good solutions to ensure reproducibility. For now, I packaged everything as a Docker compose recipe with two custom images: one constructing the embedding database and running the API, the other running the frontend.</p> <p>The final result is a relatively simple interface with a clean, minimalistic design.</p> <p><img src="/assets/img/academai/1.png" alt="Academic Search landing page" class="article-img"/></p> <p>To give it the look and feel of that ages-old friend of the academic - the green marker - I used green highlights to capture relevant passages.</p> <p><img src="/assets/img/academai/2.png" alt="Academic search search example" class="article-img"/></p> <p>Hover states highlight the passages which were retrieved from the embedding database, as well as their relative scores in terms of semantic similarity, keyword (exact search) similarity, and normalized reranking score.</p> <p><img src="/assets/img/academai/3.png" alt="Academic search highlight" class="article-img"/></p> <p>Seeing everything in action feels pretty smooth - this is a search over nearly 1,000,000 chunks over a little under 100,000 papers in a matter of seconds. Having options over the search parameters is pretty useful - knowing how many results should be retrieved and the relative weights for the semantic and keyword search gives users a finer grained control over the results.</p> <video src="/assets/img/academai/1.mp4" class="article-img" controls=""/> <h1 id="explaining-some-choices">Explaining some choices</h1> <h2 id="vector-database">Vector database</h2> <p>I quite liked the design for Weaviate - while the setup is more cumbersome than Chroma or other local vector databases, the whole process feels super smooth with minimal Docker setup <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. The one drawback that kind of annoyed me - I could not, for the life of me, get something as simple as direct access to the database itself, leading to simple queries - determining how many records are in a collection, for instance - which take a long time by having to <a href="https://docs.weaviate.io/weaviate/manage-objects/read-all-objects">read a lot of objects</a>.</p> <h2 id="embedding-and-reranking-models">Embedding and reranking models</h2> <p>This choice was fairly simple for me - I quite like HuggingFace in general so I simply went with it. The <code class="language-plaintext highlighter-rouge">SentenceTransformers</code> package makes it really simple to work with these sorts of operations and they provide support for both generic text embedding and cross-encoder models which can be used for reranking. Other options would include <a href="https://ollama.com/search"><code class="language-plaintext highlighter-rouge">Ollama</code></a>, which has fairly good feature encoding mechanisms but no cross-encoders (it is however my go to for local text generation), or <a href="https://www.langchain.com/">LangChain</a> which is a super powerful package to perform all things LLM- and agent-related. Why not go for these? They offer similar (or fewer) capabilities to <code class="language-plaintext highlighter-rouge">SentenceTransformers</code> and are less specific for the task at hand.</p> <h3 id="why-not-late-interaction">Why not late interaction?</h3> <p>The great question for me.</p> <p>There are more modern alternatives which could further improve what I have implemented here - late interaction models such as <a href="https://github.com/stanford-futuredata/ColBERT">ColBERT</a> have really good feature extraction support and <a href="https://github.com/AnswerDotAI/RAGatouille">RAGatouille</a> makes using these sorts of models a breeze.</p> <p>However, working with late-interaction is a bit of a hassle when flexibility is warranted - unlike other strategies which use chunks, late interaction uses entire documents and tends to require packaged and hard-to-port solutions for the database itself. This is, of course, not an issue if you are more or less agnostic about the database you want to use, but the options they tend to offer are, in my experience, a bit limited and hard to adapt to your favorite vector database. Additionally, because late interaction extracts features more densily, it tends to require more disk space and processing power.</p> <h2 id="api">API</h2> <p>The API was the easiest part for me - ever since <a href="https://fastapi.tiangolo.com/">FastAPI</a> arrived to the scene I have not used anything else for these sorts of applications. There are other alternatives (<a href="https://flask.palletsprojects.com/en/stable/">flask</a> is a pretty obvious one for the Pythonistas among us), but FastAPI provides a really tailored experience for APIs and the implementation and deployment is light and invariably straightforward.</p> <h1 id="wrapping-up">Wrapping up</h1> <p>This was a really fun side project - it allowed me to consolidate from an implementation perspective a lot of things which I had been reading about and which I had tested on smaller projects (particularly RAG applications). In general this is the kind of building I really appreciate - developing something that I know I will use and that solves a problem I had for a long time.</p> <h1 id="footnotes">Footnotes</h1> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>of course that there are systematic biases plaguing these vectors. We‚Äôve known this since <a href="https://www.science.org/content/article/even-artificial-intelligence-can-acquire-biases-against-race-and-gender">2017</a>, and more <a href="https://aclanthology.org/2022.gebnlp-1.20/">recent NLP models</a> or <a href="https://dl.acm.org/doi/fullHtml/10.1145/3582269.3615599">large language models</a> tend to suffer from the same issues. Biased data is biased data is biased data, no matter the algorithm.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>if you really aren‚Äôt into Docker and would rather have a super portable file with your vector database - I really think that <a href="https://www.trychroma.com/">Chroma</a> is a good solution. There are more alternatives (<a href="https://www.llamaindex.ai/">LlamaIndex</a> comes to mind as a pretty good alternative).¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="tech"/><category term="apps"/><summary type="html"><![CDATA[Semantic and hybrid retrieval for academic papers - look across>100,000 papers and find the relevant ones without losing your mind with keyword-based search.]]></summary></entry><entry><title type="html">LLMs can‚Äôt innovate</title><link href="https://josegcpa.net/blog/2025/llm-innovation/" rel="alternate" type="text/html" title="LLMs can‚Äôt innovate"/><published>2025-06-17T00:00:00+00:00</published><updated>2025-06-17T00:00:00+00:00</updated><id>https://josegcpa.net/blog/2025/llm-innovation</id><content type="html" xml:base="https://josegcpa.net/blog/2025/llm-innovation/"><![CDATA[<p>A really interesting <a href="https://mackinstitute.wharton.upenn.edu/2023/new-working-paper-finds-chatgpt-a-better-innovation-ideator-than-mba-students/">result</a>: when rating how innovative product ideas are, people tend to rate those provided by ChatGPT more highly than those provided by people.</p> <p>Equally interesting? The fact that all of these ideas - once you go through them - are of products <em>which already exist</em>.</p> <p>This raises a pretty complicated question - when we evaluate how ‚Äúcreative‚Äù or ‚Äúinnovative‚Äù an LLM is (a corollary of the forever-sought out-of-domain generalization for generative methods), what is <em>actually</em> being rated?</p> <p>To further complicate matters, and abstracting TS Kuhn‚Äôs core idea in his ‚ÄúThe Structure of Scientific Revolutions‚Äù: people have a hard time seeing radical change as a legitimate option.</p> <p>So, whatever innovation or creativity that comes from a human or an LLM is subject to a fundamental force (at least for product innovation): innovation has to be recognizable. In other words, it has to happen within a relatively narrow realm of possibilities.</p> <p>A big LLM selling point is that they <em>can</em> innovate - see <a href="https://www.technologyreview.com/2025/01/17/1110086/openai-has-created-an-ai-model-for-longevity-science/">OAI</a> and <a href="https://ai.google.dev/gemini-api/docs/gemini-for-research">Google‚Äôs</a> recent efforts in injecting their AI products in research endeavors. But when we look at hard data - <a href="https://openreview.net/forum?id=FkKBxp0FhR">Subbarao Kambhampati‚Äôs work</a> or <a href="https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf">recent papers from Apple</a> - we see clear evidence that LLMs cannot solve truly novel problems.</p> <p>So why are we seeing this grand pursuit? My bet is on the - overblown - promises that the AI industry has made to justify their existence. Sam Altman has <a href="https://fortune.com/2025/04/03/recursion-pharmaceuticals-ai-drug-discovery/">‚Äúmused‚Äù</a> that one day we might be able to ask ChatGPT to cure cancer. He also recently said that <a href="https://time.com/7205596/sam-altman-superintelligence-agi/">AI will be able to solve climate change</a> while claiming that most of Earth‚Äôs electricity <a href="https://www.yahoo.com/news/sam-altman-says-significant-fraction-130703410.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8">should eventually go to run AI</a>. I think you get the point: there is no shortage of exaggerated claims to get funding.</p> <p>However, the kind of AI contributing to research - ‚Äúsolving cancer‚Äù or ‚Äúsaving the planet‚Äù - is not the kind of AI that Altman produces. The <a href="https://alphafold.ebi.ac.uk/">AlphaFold protein structure database</a> (powered, you guessed it, by not-an-LLM <a href="https://deepmind.com/blog/article/putting-the-power-of-alphafold-into-the-worlds-hands">AlphaFold</a>) is one such kind of an AI, providing pretty good predictions of how proteins look in three-dimensions. <a href="https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/">GraphCast</a> or <a href="https://deepmind.google/science/weathernext/">WeatherNext</a> are not-an-LLM AI models which do a pretty good job at forecasting the weather. AI has also led to <a href="https://www.nature.com/articles/s41591-024-03434-4">enhanced drug development</a> or to <a href="https://www.nature.com/articles/s41467-024-44824-z">improved medical image processing</a>.</p> <p>These are narrow applications - far from what attracts billions in funding. But they are capable of creating real value using AI (or similar) applications. Even in the age of agentic AI - where we expect LLMs to interact with one another - we will still require highly accurate tools which can be used to solve real-world problems. These tools will depend largely on specialist systems capable of solving specific problems with high fidelity - not on general intelligence (whatever that may end up being).</p> <p>And mind you: LLMs can actually be useful for some tasks! If you are equipped with a working brain, curiosity, and a healthy dose of skepticism. They can be super helpful in sifting through large documents or searching for information across multiple sources. Even for a little speculative methodological banter they can be super helpful. It‚Äôs just that the ‚Äúinnovation‚Äù they can muster is only impressive when we forget that what we don‚Äôt know can fill a library (in my case many times over).</p> <p>At the end of the day AI is already doing some pretty cool things! The vast majority, however, does not depend on the narrow definition of AI as text generation. And innovation - at least in the way that we, as people, can measure it, is not really a necessity for that.</p>]]></content><author><name></name></author><category term="tech"/><summary type="html"><![CDATA[They sure can write gooder than me. But is innovation really their strongest suit? And most importantly - why does that matter?]]></summary></entry><entry><title type="html">Auto-METRICS - a proof of concept</title><link href="https://josegcpa.net/blog/2025/auto-metrics/" rel="alternate" type="text/html" title="Auto-METRICS - a proof of concept"/><published>2025-04-22T00:00:00+00:00</published><updated>2025-04-22T00:00:00+00:00</updated><id>https://josegcpa.net/blog/2025/auto-metrics</id><content type="html" xml:base="https://josegcpa.net/blog/2025/auto-metrics/"><![CDATA[<p>Recently, I received my first (obvious) LLM peer review. It was quite blatant. What‚Äôs worse: it wasn‚Äôt good - at all! Funnily enough, I had been working on something related: Auto-METRICS, a tool for automatic standardised assessment of scientific research quality in radiomics research using the <a href="https://insightsimaging.springeropen.com/articles/10.1186/s13244-023-01572-w">METRICS</a> framework.</p> <p>To show its utility, we make use of two unique, recent datasets on reproducibility in radiomics studies - Akinci D‚ÄôAntonoli et al. (2025) and Kocak (2025). Together, they feature really good set of METRICS ratings - for different levels of expertise and training - for more than 50 publications. This allowed us to systematically compare human and LLM raters.</p> <p>The main takeaways:</p> <ul> <li>Human raters agree with LLMs at the same rate that they agree with other human raters ‚úÖ</li> <li>Prompt iterations: clarifying radiomics guidelines can lead to better agreement with human raters. However these improvements were quite limited! üìà</li> <li>Too nice: LLM ratings tended to be slightly higher than those offered by human raters üòá</li> </ul> <p>I tested our tool - Auto-METRICS - <a href="https://autometrics.josegcpa.net/">here</a> (all you need is a <em>free</em> Google Gemini API key) and found it really helpful to get an initial assessment for METRICS which I can easily confirm. The key? Enhance, don‚Äôt replace - having good initial ratings was super helpful in getting a final, human-based classification.</p> <p>Curious? Read more about Auto-METRICS at <a href="https://www.medrxiv.org/content/10.1101/2025.04.22.25325873v1">medRxiv</a>.</p>]]></content><author><name></name></author><category term="apps"/><summary type="html"><![CDATA[Automatic assessment of methodological quality in radiomics research]]></summary></entry><entry><title type="html">How to develop a radiomics signature</title><link href="https://josegcpa.net/blog/2024/esgar-radiomics/" rel="alternate" type="text/html" title="How to develop a radiomics signature"/><published>2024-12-11T00:00:00+00:00</published><updated>2024-12-11T00:00:00+00:00</updated><id>https://josegcpa.net/blog/2024/esgar-radiomics</id><content type="html" xml:base="https://josegcpa.net/blog/2024/esgar-radiomics/"><![CDATA[<p>In 2024, I participated as faculty in the ‚ÄúHow to develop a radiomics signature‚Äù course organised for the <a href="https://esgar.org/">European Society of Gastrointestinal and Abdominal Radiology</a>. Here, I gave a short presentation on machine-learning model development and tuning, as well as two practical programming sessions: one on radiomic feature extraction and the other on model development and (fine-)tuning. Both are <a href="https://github.com/CCIG-Champalimaud/waw-tace-radiomics">freely available</a>.</p>]]></content><author><name></name></author><category term="teaching"/><summary type="html"><![CDATA[Presentation and workshop on applying machine-learning to radiology data to medical doctors + biomedical researchers]]></summary></entry><entry><title type="html">Some notes on programming and coding with AI assistants</title><link href="https://josegcpa.net/blog/2024/thoughts-ai-assistant/" rel="alternate" type="text/html" title="Some notes on programming and coding with AI assistants"/><published>2024-10-10T00:00:00+00:00</published><updated>2024-10-10T00:00:00+00:00</updated><id>https://josegcpa.net/blog/2024/thoughts-ai-assistant</id><content type="html" xml:base="https://josegcpa.net/blog/2024/thoughts-ai-assistant/"><![CDATA[<p>Recently, I have been wondering about AI coding assistants (AICA). A lot of what I do day-to-day is coding, so having a convenient assistant is good.</p> <p>I have tried a few and my general impression is simple: it works well, but it frequently introduces bugs which I have to fix. Ultimately, I have a hard time understanding whether I see an improvement in my coding ability or not - on one hand I can code faster; on the other I have to fix more bugs.</p> <p>So I took a quick dive into the literature (some early works trying to predict productivity gains are available, but, alas!, behind these keystrokes lies an empiricist):</p> <ul> <li>GitClear conducted their own study into this, showing that post-AICA code has a higher churn rate (i.e. code requiring maintenance further down the line) <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></li> <li>Some works focus on understanding whether AICA solutions are good without verification - they are not<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> <sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>, with a recent work showing that only 30% of code suggestions are accepted <sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup></li> <li>In terms of productivity, the same work claims a 30% increase in productivity (!) <sup id="fnref:4:1" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>. However, productivity is measured as the acceptance rate of AICA suggestions, which, despite being a somewhat dumb metric, is associated with <em>perceived</em> productivity <sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup></li> </ul> <p>From other works on the impact of coding assistants on productivity, we get a heterogenous picture:</p> <ol> <li>a study asking participants to implement an HTTP server in JS as quickly as possible showed that people using AICA were able to get the job done 50% faster <sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup></li> <li>a study from Uplevel (on 800 of its customers) saw a 40% increase (!) in bugs with very little impact on efficiency (PR cycles reduced by 1.7 minutes) and no impact on burnout rate <sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup></li> <li>a study shows an increase in PRs but also a decrease in build success rates <sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup></li> <li>a study surveying developers determined that they are generally positive about AICA, but are somewhat fearful that more junior developers won‚Äôt have the opportunity to own up to their skills if they rely too heavily on these tools; additionally they were more keen to boast its efficiency boosts when tasks are easy or repetitive (i.e. the usual boilerplate code) which leaves more time for learning and creative thinking <sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup></li> <li>a study on developers in the public sector highlights a similar trend, with developers feeling that they have more time to focus on more meaningful and rewarding tasks if they use AICA <sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup></li> <li>showed that GH CoPilot has a more positive impact when used by experts and that it can be harmful for novices as it suggests buggy/complicated code which novices cannot fully comprehend <sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">11</a></sup></li> </ol> <p>My take on this: it can be useful, but a lot of the benefits are overstated and novice developers benefit greatly from steering clear from it while learning. Benefits appear to lie in a speed-quality tradeoff curve, so each position will have a specific position on this.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p><a href="https://www.gitclear.com/coding_on_copilot_data_shows_ais_downward_pressure_on_code_quality">Coding on Copilot: 2023 Data Suggests Downward Pressure on Code Quality</a>¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2209.01766">Exploring the Verifiability of Code Generated by GitHub Copilot</a>¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3" role="doc-endnote"> <p><a href="https://dl.acm.org/doi/pdf/10.1145/3558489.3559072">Assessing the Quality of GitHub Copilot‚Äôs Code Generation</a>¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4" role="doc-endnote"> <p><a href="https://arxiv.org/pdf/2306.15033">Sea Change in Software Development: Economic and Productivity Analysis of the AI-Powered Developer Lifecycle</a>¬†<a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:4:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:5" role="doc-endnote"> <p><a href="https://cacm.acm.org/research/measuring-github-copilots-impact-on-productivity/">Measuring GitHub Copilot‚Äôs Impact on Productivity</a>¬†<a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:6" role="doc-endnote"> <p><a href="https://arxiv.org/pdf/2302.06590">The Impact of AI on Developer Productivity: Evidence from GitHub Copilot</a>¬†<a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:7" role="doc-endnote"> <p><a href="https://resources.uplevelteam.com/gen-ai-for-coding">Can GenAI Actually Improve Developer Productivity?</a>¬†<a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:8" role="doc-endnote"> <p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4945566">The Effects of Generative AI on High-Skilled Work: Evidence from Three Field Experiments with Software Developers</a>¬†<a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:9" role="doc-endnote"> <p><a href="https://pure.tue.nl/ws/portalfiles/portal/320756654/MTP_thesis_report_Joella_Schouwenaar.pdf">Understanding the impact of an AI coding assistant, GitHub‚Äôs Copilot, on developers and their work experiences</a>¬†<a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:10" role="doc-endnote"> <p><a href="https://arxiv.org/pdf/2409.17434">Harnessing the Potential of Gen-AI Coding Assistants in Public Sector Software Development</a>¬†<a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:11" role="doc-endnote"> <p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0164121223001292">GitHub Copilot AI pair programmer: Asset or Liability?</a>¬†<a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="tech"/><summary type="html"><![CDATA[Most IDEs come pre-packaged with their own AI assistant for coding. But who should use them?]]></summary></entry><entry><title type="html">Para onde foi o microbioma do cancro?</title><link href="https://josegcpa.net/blog/2023/microbioma/" rel="alternate" type="text/html" title="Para onde foi o microbioma do cancro?"/><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><id>https://josegcpa.net/blog/2023/microbioma</id><content type="html" xml:base="https://josegcpa.net/blog/2023/microbioma/"><![CDATA[]]></content><author><name></name></author><category term="science"/><summary type="html"><![CDATA[Um artigo cient√≠fico publicado em 2020 prometia revolucionar o diagn√≥stico de cancro. Quando outras equipas n√£o obtiveram os mesmos resultados, foram lan√ßadas d√∫vidas sobre a investiga√ß√£o e sobre a empresa que se fundou inspirada nas conclus√µes.]]></summary></entry><entry><title type="html">As ferramentas mudam mas as lutas continuam ‚Äì a Intelig√™ncia Artificial e o que quer dizer para o teu trabalho</title><link href="https://josegcpa.net/blog/2023/trabalho/" rel="alternate" type="text/html" title="As ferramentas mudam mas as lutas continuam ‚Äì a Intelig√™ncia Artificial e o que quer dizer para o teu trabalho"/><published>2023-07-14T00:00:00+00:00</published><updated>2023-07-14T00:00:00+00:00</updated><id>https://josegcpa.net/blog/2023/trabalho</id><content type="html" xml:base="https://josegcpa.net/blog/2023/trabalho/"><![CDATA[]]></content><author><name></name></author><category term="essay"/><summary type="html"><![CDATA[Os novos modelos de Intelig√™ncia Artificial captaram a aten√ß√£o de tudo e todos. Com a sua capacidade para gerar texto como nunca antes visto, surgiram medos de que possam destruir empregos em massa. Ser√° essa amea√ßa real? Ou os pressupostos desse medo s√£o mais sociais e pol√≠ticos do que tecnol√≥gicos? Escrito com Jo√£o Gabriel Ribeiro]]></summary></entry><entry><title type="html">Where is my mind: notas sobre leitura mental e Intelig√™ncia Artificial</title><link href="https://josegcpa.net/blog/2023/mind/" rel="alternate" type="text/html" title="Where is my mind: notas sobre leitura mental e Intelig√™ncia Artificial"/><published>2023-05-08T00:00:00+00:00</published><updated>2023-05-08T00:00:00+00:00</updated><id>https://josegcpa.net/blog/2023/mind</id><content type="html" xml:base="https://josegcpa.net/blog/2023/mind/"><![CDATA[]]></content><author><name></name></author><category term="science"/><summary type="html"><![CDATA[Nos √∫ltimos seis meses, por duas vezes tivemos momentos em que os cabe√ßalhos das not√≠cias anunciavam algo sobremaneira fascinante: a Intelig√™ncia Artificial tornou-se capaz de ler a mente humana. Contudo, h√° uma dist√¢ncia entre os cabe√ßalhos e a realidade.]]></summary></entry></feed>