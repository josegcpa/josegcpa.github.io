<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Implementing academ.ai, a local retrieval system for academic papers | Jos√© G. de Almeida</title> <meta name="author" content="Jos√© G. de Almeida"/> <meta name="description" content="Semantic and hybrid retrieval for academic papers - look across>100,000 papers and find the relevant ones without losing your mind with keyword-based search. " /> <meta name="keywords" content="computational-biology, deep-learning, artificial-intelligence, machine-learning, computer-vision, statistical-modelling"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üíª</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://josegcpa.net/blog/2025/academai/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/monokai.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <div id="loadingDiv"></div> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title" href="/"><span class="font-weight-bold">Jos√©¬†</span>G.¬†de Almeida</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">flow </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container container-all"> <div class="post"> <header class="post-header"> <h1 class="card-title">Implementing academ.ai, a local retrieval system for academic papers</h1> <p class="post-meta"><b>September 16, 2025</b> <a class="badge" href="/blog/tag/tech"> tech</a> <a class="badge" href="/blog/tag/apps"> apps</a> </p> <p class="post-description">Semantic and hybrid retrieval for academic papers - look across &gt;100,000 papers and find the relevant ones without losing your mind with keyword-based search. </p> </header> <article class="post-content"> <p><strong>tl;dr:</strong> I developed a semantic search engine that runs locally and comes packaged with a nice and simple interface. I used it to look for papers across bioRxiv and medRxiv. The code is open source and is available <a href="https://github.com/josegcpa/academic-search" target="_blank" rel="noopener noreferrer">here</a>.</p> <h1 id="context">Context</h1> <p>An issue which a lot of researchers run into are the relatively outdated search engines powering academic manuscript databases. I felt a bit frustrated with how the relatively poor support for semantic searchers: oftentimes, I got stuck having to think of the ideal keywords to retrieve the papers I was looking for. On the other hand, popular search engines have widely adopted everything related to semantic search (the comically subpar ‚ÄúAI summaries‚Äù will remain undiscussed here).</p> <p>While this is perhaps not something that a lot of people contend with, the usual academic manuscript search rests on a core principle: exactness. If people conduct a proper meta-analysis or meta-review, they will oftentimes post long, intractable queries which are understood by search engines but which can be relatively hard to comprehend. Words can be, for better or worse, both polysemous - where the same word refers to multiple things - and synonymous - where the same thing can be referred to by multiple words. This means that queries have to be highly sensitive (they have to retrieve all papers related to a given field) but also specific (they have to filter out as many unrelated papers as possible).</p> <h2 id="thinking-about-retrieval">Thinking about retrieval</h2> <p>With this in mind I started thinking - can this be improved?</p> <p><strong>Manual query construction</strong> can be quite time-consuming and error-prone. I remember reviewing some meta-analysis papers which missed key papers because they did not think about specific keywords. Some improvements can be considered: preprocessing words and reducing them to their stems (think of the word ‚Äúrunning‚Äù and ‚Äúruns‚Äù - they are both reduced to ‚Äúrun‚Äù) or enriching the query with synonyms. However, we once again run into the issue of exactness as this changes the sensitivity and specificity of our run.</p> <p>I have toyed with <strong>automatic query generation</strong> using LLMs and this works to a good extent. In other words, you can use an LLM to construt queries which can be used by your favourite search engine using a mixture of clever prompting and structured outputs for parsing at a later stage. However, this still relies on some form of exactness which is hard to verify. Different and shifting word meanings can have an impact on how capable LLMs are of building these sorts of queries.</p> <p>We can also consider approaches which first retrieve a very high number of results (high sensitivity) and perform some metadata-based filtering to increase the specificity. However, this is just an iteration of query generation or construction, with or without filters.</p> <p>One of the features I enjoy the most about <strong>modern chatbots</strong> is their ability to search the web. What they do is fairly simple and is, essentially, an iteration of tool use with <strong>automatic query generation</strong>. A search is triggered because the chatbot predicts that its knowledge base is insufficient to provide an accurate answer to the user‚Äôs message, which is converted into a search query. The digestion that happens afterwards, where the relevant information is extracted, is really the best part as it provides some structure to the chatbot‚Äôs output. Now, don‚Äôt get me wrong - this summarization feature is not the most accurate, but it helps me filter through the clutter of web search results.</p> <p>Other recent methods also include the <strong>automatic extraction of metadata</strong> from documents or webpages to facilitate search - however, this is also hindered by having to specify the metadata fields to extract, which can change with time.</p> <p>The ideal would be a method which can take characterize the <em>meaning</em> of a query and retrieve relevant documents to it. This is what <strong>semantic search</strong> is all about.</p> <h2 id="its-all-semantics-non-derogatory">It‚Äôs all semantics (non-derogatory)</h2> <p>Generally, the whole point of <strong>retrieval</strong> is to produce a set of results which are related to a given query. In other words, a good set of results is characterized by proximity to the query in terms of meaning, not exactness. This is where <strong>semantic search</strong> comes in.</p> <p>Semantic search - as it is performed currently - usually uses a natural language processing (NLP) model to quantify a sentence or document as a vector (which can also be referred to as an embedding). This vector should represent (using typically a large number of dimensions) the meaning of that sentence or document. This means that two semantically similar but distinct phrases - ‚Äúthe queen of England‚Äù and ‚Äúthe monarch of the United Kingdom‚Äù - should yield similar vectors. <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p> <p>The options to compute embeddings are manifold and generally involve generating one embedding for each token (which typically corresponds to a word or to a part of a word). The simpler options are models similar to <a href="https://research.google/blog/open-sourcing-bert-state-of-the-art-pre-training-for-natural-language-processing/" target="_blank" rel="noopener noreferrer">BERT</a>, which use a model training paradigm combining masked word prediction (a word at random is masked and predicted from the rest of the sentence) with next sentence prediction (the model predicts whether a given sentence is the next sentence in a document) to guarantee both grammatical and semantic coherence. Other popular methods include <a href="https://arxiv.org/abs/2004.12832" target="_blank" rel="noopener noreferrer">ColBERT</a> - which is similar to BERT but uses a different training paradigm where the model is trained to retrieve the best matching tokens or words from query and document embeddings - and any old LLM which can be used to generate embeddings for a set of tokens. These models yield one embedding for each token. These are aggregated in some way (average or maximum pooling, classification tokens) to yield a single query/sentence/document embedding.</p> <p>On top of this, exact matching (or a <a href="https://www.geeksforgeeks.org/nlp/what-is-bm25-best-matching-25-algorithm/" target="_blank" rel="noopener noreferrer">variation</a>) can still be used - the combination of multiple retrieval methods is typically referred to as ‚Äúhybrid‚Äù search, and most search engines tend towards hybrid searches. While somewhat odd, it starts to make sense once you realise that a lot of people (myself included) use search terms in a relatively telegraphic manner: if I want to search for papers on caffeine‚Äôs impact on the onset of Alzheimer‚Äôs disease, I will simply type ‚Äúcaffeine alzheimer‚Äôs onset‚Äù. Combining both exact matching and semantic search can yield a search that feels more natural and that better works for everyone.</p> <h1 id="implementing-a-local-retrieval-system">Implementing a local retrieval system</h1> <p><strong>Note:</strong> all of this is somewhat similar to what Semantic Scholar does through their API services - I am not sure how their search functionalities work, but I would assume they rely on keyword matching with a minimal amount of semantic search. What I present below is, in essence, the hacker‚Äôs version of that with fewer resources/papers. In any case this is scalable using the <a href="https://www.semanticscholar.org/product/api/tutorial" target="_blank" rel="noopener noreferrer">Semantic Scholar data API</a>.</p> <h2 id="1-getting-the-data">1. Getting the data</h2> <p>To implement a local retrieval system for academic literature search, we first need to get a hold of a dataset of academic abstracts. Luckily, both <a href="https://www.biorxiv.org/" target="_blank" rel="noopener noreferrer">bioRxiv</a> and <a href="https://www.medrxiv.org/" target="_blank" rel="noopener noreferrer">medRxiv</a> have very handy APIs for this. Using programmatic access and some minimal parallelization, we can retrieve a large number of abstracts in a matter of minutes. I stored these in a <a href="https://sqlite.org/index.html" target="_blank" rel="noopener noreferrer">SQLite</a> database so everything is standardized.</p> <h2 id="2-embedding-the-data">2. Embedding the data</h2> <p>This is the time consuming bit. I quite like the <a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">HuggingFace</a> stack with <a href="https://www.sbert.net/" target="_blank" rel="noopener noreferrer">SentenceTransformers</a>, so I used that for embedding my abstract chunks (sentences or pieces of the abstracts). Given that I am running all of this locally and that I don‚Äôt have a GPU, I used a<a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2" target="_blank" rel="noopener noreferrer">really small model</a> for this. For the embedding database/retrieval workhorse I made use of <a href="https://weaviate.io/" target="_blank" rel="noopener noreferrer">Weaviate</a>, a pretty powerful solution that is open source and guarantees fast retrieval while also supporting multiple types of search and metadata filtering.</p> <p>I defined two separate databases (or collections): one for abstracts and the other for abstract chunks. This allowed me to have databases which are less disk space-intensive while guaranteeing that I can retrieve the chunks I need. These chunks are cross-referenced to their abstracts, so it is really easy to retrieve the abstracts corresponding to the chunks.</p> <h2 id="3-defining-an-api">3. Defining an API</h2> <p>The simplest API I could define was one with three endpoints:</p> <ol> <li> <code class="language-plaintext highlighter-rouge">/health</code> - a simple health check endpoint</li> <li> <code class="language-plaintext highlighter-rouge">/query</code> - a search endpoint which takes a query and returns a structured list of relevant abstract chunks</li> <li> <code class="language-plaintext highlighter-rouge">/count</code> - a count endpoint which takes a query and returns the number of relevant abstract chunks by source (i.e. <code class="language-plaintext highlighter-rouge">medrxiv</code> or <code class="language-plaintext highlighter-rouge">biorxiv</code>)</li> </ol> <p>The technical stack here is very classic if you are used to Python - <a href="https://fastapi.tiangolo.com/" target="_blank" rel="noopener noreferrer">FastAPI</a> and <a href="https://pydantic-docs.helpmanual.io/" target="_blank" rel="noopener noreferrer">Pydantic</a> for data models.</p> <h2 id="4-re-ranking-results">4. Re-ranking results</h2> <p>Before we can display the results to the user, we need to re-rank them. While semantic search tends to work well for the general task of retrieving similar results, distances are not always a good proxy for relevance. This is where result re-ranking comes into play: using a set of models known as cross-encoders, which produce a similarity score for two queries (in this case, the abstract chunk and the user‚Äôs query), we can re-rank the results based on actual similarity to the query.</p> <p>Once again, SentenceTransformers came to the rescue - this time with <a href="https://huggingface.co/cross-encoder/ms-marco-TinyBERT-L2-v2" target="_blank" rel="noopener noreferrer">a really small cross-encoder model</a>.</p> <h2 id="5-making-a-pretty-thing">5. Making a pretty thing</h2> <p>I am a fan of minimal web development, so I coded a small frontend using the classic HTML/CSS/JS stack. I know the cool kids are all using React and Tailwind, but it would be an overkill for such a small project. Plus, if it becomes a necessity I can always switch to a more modern stack.</p> <h1 id="using-a-local-retrieval-system">Using a local retrieval system</h1> <p>Given that these are, in essence, a small set of micro-services, there are good solutions to ensure reproducibility. For now, I packaged everything as a Docker compose recipe with two custom images: one constructing the embedding database and running the API, the other running the frontend.</p> <p>The final result is a relatively simple interface with a clean, minimalistic design.</p> <p><img src="/assets/img/academai/1.png" alt="Academic Search landing page" class="article-img"></p> <p>To give it the look and feel of that ages-old friend of the academic - the green marker - I used green highlights to capture relevant passages.</p> <p><img src="/assets/img/academai/2.png" alt="Academic search search example" class="article-img"></p> <p>Hover states highlight the passages which were retrieved from the embedding database, as well as their relative scores in terms of semantic similarity, keyword (exact search) similarity, and normalized reranking score.</p> <p><img src="/assets/img/academai/3.png" alt="Academic search highlight" class="article-img"></p> <p>Seeing everything in action feels pretty smooth - this is a search over nearly 1,000,000 chunks over a little under 100,000 papers in a matter of seconds. Having options over the search parameters is pretty useful - knowing how many results should be retrieved and the relative weights for the semantic and keyword search gives users a finer grained control over the results.</p> <video src="/assets/img/academai/1.mp4" class="article-img" controls=""></video> <h1 id="explaining-some-choices">Explaining some choices</h1> <h2 id="vector-database">Vector database</h2> <p>I quite liked the design for Weaviate - while the setup is more cumbersome than Chroma or other local vector databases, the whole process feels super smooth with minimal Docker setup <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. The one drawback that kind of annoyed me - I could not, for the life of me, get something as simple as direct access to the database itself, leading to simple queries - determining how many records are in a collection, for instance - which take a long time by having to <a href="https://docs.weaviate.io/weaviate/manage-objects/read-all-objects" target="_blank" rel="noopener noreferrer">read a lot of objects</a>.</p> <h2 id="embedding-and-reranking-models">Embedding and reranking models</h2> <p>This choice was fairly simple for me - I quite like HuggingFace in general so I simply went with it. The <code class="language-plaintext highlighter-rouge">SentenceTransformers</code> package makes it really simple to work with these sorts of operations and they provide support for both generic text embedding and cross-encoder models which can be used for reranking. Other options would include <a href="https://ollama.com/search" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">Ollama</code></a>, which has fairly good feature encoding mechanisms but no cross-encoders (it is however my go to for local text generation), or <a href="https://www.langchain.com/" target="_blank" rel="noopener noreferrer">LangChain</a> which is a super powerful package to perform all things LLM- and agent-related. Why not go for these? They offer similar (or fewer) capabilities to <code class="language-plaintext highlighter-rouge">SentenceTransformers</code> and are less specific for the task at hand.</p> <h3 id="why-not-late-interaction">Why not late interaction?</h3> <p>The great question for me.</p> <p>There are more modern alternatives which could further improve what I have implemented here - late interaction models such as <a href="https://github.com/stanford-futuredata/ColBERT" target="_blank" rel="noopener noreferrer">ColBERT</a> have really good feature extraction support and <a href="https://github.com/AnswerDotAI/RAGatouille" target="_blank" rel="noopener noreferrer">RAGatouille</a> makes using these sorts of models a breeze.</p> <p>However, working with late-interaction is a bit of a hassle when flexibility is warranted - unlike other strategies which use chunks, late interaction uses entire documents and tends to require packaged and hard-to-port solutions for the database itself. This is, of course, not an issue if you are more or less agnostic about the database you want to use, but the options they tend to offer are, in my experience, a bit limited and hard to adapt to your favorite vector database. Additionally, because late interaction extracts features more densily, it tends to require more disk space and processing power.</p> <h2 id="api">API</h2> <p>The API was the easiest part for me - ever since <a href="https://fastapi.tiangolo.com/" target="_blank" rel="noopener noreferrer">FastAPI</a> arrived to the scene I have not used anything else for these sorts of applications. There are other alternatives (<a href="https://flask.palletsprojects.com/en/stable/" target="_blank" rel="noopener noreferrer">flask</a> is a pretty obvious one for the Pythonistas among us), but FastAPI provides a really tailored experience for APIs and the implementation and deployment is light and invariably straightforward.</p> <h1 id="wrapping-up">Wrapping up</h1> <p>This was a really fun side project - it allowed me to consolidate from an implementation perspective a lot of things which I had been reading about and which I had tested on smaller projects (particularly RAG applications). In general this is the kind of building I really appreciate - developing something that I know I will use and that solves a problem I had for a long time.</p> <h1 id="footnotes">Footnotes</h1> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>of course that there are systematic biases plaguing these vectors. We‚Äôve known this since <a href="https://www.science.org/content/article/even-artificial-intelligence-can-acquire-biases-against-race-and-gender" target="_blank" rel="noopener noreferrer">2017</a>, and more <a href="https://aclanthology.org/2022.gebnlp-1.20/" target="_blank" rel="noopener noreferrer">recent NLP models</a> or <a href="https://dl.acm.org/doi/fullHtml/10.1145/3582269.3615599" target="_blank" rel="noopener noreferrer">large language models</a> tend to suffer from the same issues. Biased data is biased data is biased data, no matter the algorithm.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">‚Ü©</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>if you really aren‚Äôt into Docker and would rather have a super portable file with your vector database - I really think that <a href="https://www.trychroma.com/" target="_blank" rel="noopener noreferrer">Chroma</a> is a good solution. There are more alternatives (<a href="https://www.llamaindex.ai/" target="_blank" rel="noopener noreferrer">LlamaIndex</a> comes to mind as a pretty good alternative).¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">‚Ü©</a></p> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-4"> <div class="social"> <div class="contact-note"> Get in touch. </div> <div class="contact-icons"> <a href="mailto:%6A%6F%73%65.%61%6C%6D%65%69%64%61@%72%65%73%65%61%72%63%68.%66%63%68%61%6D%70%61%6C%69%6D%61%75%64.%6F%72%67" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-1887-0157" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=4u2Ht8AAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/josegcpa" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/josegcpa" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> </div> </div> <div class="footer-text"> ¬© 2026 Jos√© G. de Almeida. Last updated: January 09, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-YN06HGHSB4"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-YN06HGHSB4");</script> <script>const loadingDiv=document.getElementById("loadingDiv");loadingDiv.style.opacity=0,setTimeout(function(){loadingDiv.style.display="none"},500);</script> </body> </html>